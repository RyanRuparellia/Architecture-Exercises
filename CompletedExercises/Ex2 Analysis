Judging by the outputted file, it seems clear that this model is not a very powerful one. On the whole, it is very prone to repetition and will often repeat itself and add new lines. 
Some notable instances of errors:

Error: outputting \n\n\n etc.
Prompt: 'Ready, set, go!', he said
Generated (len=20, temp=0.5, top_k=10): 'Ready, set, go!', he said. "I'm just going to go to the airport and I'll do it."




etc.
Generation Time: 3.7958 seconds
Token Count: 17
----------------------------------------
Error: Repeating a phrase
Prompt: 'Ready, set, go!', he said
Generated (len=20, temp=0.5, top_k=50): 'Ready, set, go!', he said.



"I'm sure you're going to get a little bit of a kick out of it," he said.
"I'm sure you're going to get a little bit of a kick out of it."
"I'm sure you're going to get a little bit of a kick out of it," he said.
"I'm sure you're going to get a little bit of a kick out of it," he said.
"I'm sure you're going to get a little bit of a kick out of it," he said.
etc.
Generation Time: 3.6677 seconds
Token Count: 191
--------------------------------------
Error: Unknown characters
Prompt: The sky is blue because
Generated (len=20, temp=1.0, top_k=10): The sky is blue because it․s the lightest place on earth,․s a place with light.”


․․․․․․․․․․․․․
․․․․․․․․․․․
․․․․․․․․․․․
․․․․․․․․․․․․․․․․․․․
․․․․․․․․․․․․․․․․․․․․․․
․․․․․․․․․․․․․․․․․․․․․․․․․․․․․․․․․․․․․�
Generation Time: 3.2632 seconds
Token Count: 21
----------------------------------------


The average generation time was 11.4611 seconds, which is relatively good. One problem with my code is that when I set a value for max_length, it seems to be ignored in favour of max_new_tokens. This, at least I believe, cannot be helped as it is more of a problem with pipeline itself, but I'll have to keep in mind to only set max_new_tokens as a parameter.

The best results were generated under the following conditions:
  1. As the temperature approached 2, the results became much more literate. For example:
        Prompt: 'Ready, set, go!', he said
        Generated (len=100, temp=1.5, top_k=100): 'Ready, set, go!', he said. "You need not be worried, either. He would just find his man when it gets tough anyway."
        It broke his confidence on August 23. 'Wait, there was a thief. Don't hit, or stay away,' Dr. Boulga added. "Go up. Have at another level. Whatever. Call that boy,'" Dr. Boulga said through a thick line while standing nearby. 'Be prepared!'', Stip, whispered before him. "I look to a lot of kids all day I'll never feel worried the first time someone picks me up or calls home. Let's see if something I'd rather have."
        Dr. Boulga shook her head solemnly. Instead of helping me walk by he nodded his head lightly upon the corner of his mouth and into a chair just above him with him behind his seat. 'Good kid, nice man,' he whispered he wished in his small voice it wasn't the smell, that he wished when the news broke that Dr Rommel was back off for his lesson and called the group a little more careful "Shit,' as one, you know?"
        Stip closed-handed. Rather than sit quietly, his body turned up from the front window; what a difference would be at
        Generation Time: 8.8258 seconds
        Token Count: 198
    Interestingly, with a high temperature, the model generated actual text even for the nonsensical prompt:
        Prompt: ?DfsafaefgGHR
        Generated (len=100, temp=1.5, top_k=100): ?DfsafaefgGHRD-O0QVJtN0Q -4 124512 +18127700
        
        The official forums started today (23Aug 11 17 17:43:27 GMT)+197666245053
        Generation Time: 0.7682 seconds
        Token Count: 14
    It seems to have just regurgitated what was in its data set. This brings me to the second condition. It was also a smaller output for some reason.

  2. The more relevant the prompt was to the datasets that the model was trained on, the more coherent the output. For example:
